<!DOCTYPE html><html lang="zh_CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>tiaotiaohuの98 | tiaotiaohuの98</title><meta name="author" content="tiaotiaohu"><meta name="copyright" content="tiaotiaohu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="聚类 - wolai 笔记                                                                                                                                     上篇主要介绍了一种机器学习的通用">
<meta property="og:type" content="website">
<meta property="og:title" content="tiaotiaohuの98">
<meta property="og:url" content="https://tiaotiaohu98.github.io/machineLearning/ML/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(11)--%E8%81%9A%E7%B1%BB.html">
<meta property="og:site_name" content="tiaotiaohuの98">
<meta property="og:description" content="聚类 - wolai 笔记                                                                                                                                     上篇主要介绍了一种机器学习的通用">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">
<meta property="article:published_time" content="2022-08-31T13:58:43.192Z">
<meta property="article:modified_time" content="2022-08-31T13:58:43.192Z">
<meta property="article:author" content="tiaotiaohu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://tiaotiaohu98.github.io/machineLearning/ML/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(11)--%E8%81%9A%E7%B1%BB"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: tiaotiaohu","link":"链接: ","source":"来源: tiaotiaohuの98","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'tiaotiaohuの98',
  isPost: false,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-08-31 21:58:43'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/favicon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">articles</div><div class="length-num">18</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">tags</div><div class="length-num">1</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">categories</div><div class="length-num">1</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/machineLearning/machineLearning"><i class="fa-fw fas fa-machineLearning"></i><span> 机器学习</span></a></li><li><a class="site-page child" href="/KG/KG"><i class="fa-fw fas fa-KG"></i><span> 知识图谱</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/myimg/"><i class="fa-fw fas fa-myimg"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">tiaotiaohuの98</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/machineLearning/machineLearning"><i class="fa-fw fas fa-machineLearning"></i><span> 机器学习</span></a></li><li><a class="site-page child" href="/KG/KG"><i class="fa-fw fas fa-KG"></i><span> 知识图谱</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/myimg/"><i class="fa-fw fas fa-myimg"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="page"><h1 class="page-title"></h1><div id="article-container"><!DOCTYPE html>
<html lang="zh-Hans-CN">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <link rel="stylesheet" type="text/css" href="css/modern-norm.min.css" />
    <link rel="stylesheet" type="text/css" href="css/prism.min.css" />
    <link rel="stylesheet" type="text/css" href="css/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="css/wolai.css" />
    <title>聚类 - wolai 笔记</title>
    <link rel="shortcut icon"
        href="data:image/svg+xml,%3Csvg xmlns=&apos;http://www.w3.org/2000/svg&apos; viewBox=&apos;0 0 800 800&apos;%3E%3Cdefs%3E%3Cstyle%3E.cls-1%7Bfill:%23fff;%7D%3C/style%3E%3C/defs%3E%3Cg%3E%3Cpath class=&apos;cls-1&apos; d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Z&apos;/%3E%3Cpath d=&apos;M610.08,0c66,0,90,6.88,114.13,19.79a134.62,134.62,0,0,1,56,56l2.28,4.4C793.93,103,800,127.88,800,189.92V610.08l-.08,11.56c-.78,57.38-7.58,79.89-19.71,102.57a134.62,134.62,0,0,1-56,56l-4.4,2.28C697,793.93,672.12,800,610.08,800H189.92l-11.56-.08c-57.38-.78-79.89-7.58-102.57-19.71a134.62,134.62,0,0,1-56-56l-2.28-4.4C6.44,697.75.4,673.72,0,616L0,189.92c0-66,6.88-90,19.79-114.13a134.62,134.62,0,0,1,56-56l4.4-2.28C102.25,6.44,126.28.4,184,0Zm4.72,88.9H185.2L172.42,89c-32.78.62-43.68,3.24-54.71,9.14a45.84,45.84,0,0,0-19.54,19.54c-6.61,12.36-9.11,24.55-9.27,67.49V614.8L89,627.58c.62,32.78,3.24,43.68,9.14,54.71a45.84,45.84,0,0,0,19.54,19.54c12.36,6.61,24.55,9.11,67.49,9.27H610.08c46.79,0,59.41-2.44,72.21-9.28a45.84,45.84,0,0,0,19.54-19.54c6.61-12.36,9.11-24.55,9.27-67.49V189.92c0-46.79-2.44-59.41-9.28-72.21a45.84,45.84,0,0,0-19.54-19.54C669.93,91.56,657.74,89.06,614.8,88.9ZM233.33,493.33A73.34,73.34,0,1,1,160,566.67,73.35,73.35,0,0,1,233.33,493.33Z&apos;/%3E%3C/g%3E%3C/svg%3E">
    </link>
</head>

<body>
    <header>
        <div class="image"></div>
        <div class="title">
            <div class="banner">
                <div class="icon"></div>
            </div>
            <div data-title="聚类" class="main-title"></div>
        </div>
    </header>
    <article>
        <div id="a9JsTiJe3dftGpx69aemAD" class="wolai-block wolai-text">
            <div><span class="inline-wrap">上篇主要介绍了一种机器学习的通用框架--集成学习方法，首先从准确性和差异性两个重要概念引出集成学习“</span><span
                    class="inline-wrap"><b>好而不同</b></span><span
                    class="inline-wrap">”的四字真言，接着介绍了现阶段主流的三种集成学习方法：AdaBoost、Bagging<span class="jill"></span>及<span
                        class="jill"></span>Random Forest，AdaBoost<span
                        class="jill"></span>采用最小化指数损失函数迭代式更新样本分布权重和计算基学习器权重，Bagging<span
                        class="jill"></span>通过自助采样引入样本扰动增加了基学习器之间的差异性，随机森林则进一步引入了属性扰动，最后简单概述了集成模型中的三类结合策略：平均法、投票法及学习法，其中<span
                        class="jill"></span>Stacking<span
                        class="jill"></span>是学习法的典型代表。本篇将讨论无监督学习中应用最为广泛的学习算法--聚类。</span></div>
        </div>
        <h1 id="iJUbhgr5Y8zYmocGeBBmzP" class="wolai-block"><span class="inline-wrap"><b>10、聚类算法</b></span></h1>
        <div id="nPGiZec1n1ZdNcypw56Yru" class="wolai-block wolai-text">
            <div><span class="inline-wrap">聚类是一种经典的</span><span class="inline-wrap"><b>无监督学习</b></span><span
                    class="inline-wrap">方法，</span><span
                    class="inline-wrap"><b>无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律</b></span><span
                    class="inline-wrap">，即不依赖于训练数据集的类标记信息。聚类则是试图将数据集的样本划分为若干个互不相交的类簇，从而每个簇对应一个潜在的类别。</span></div>
        </div>
        <div id="6PsEf4AfS1VyRJuSVfjG6G" class="wolai-block wolai-text">
            <div><span class="inline-wrap">聚类直观上来说是将相似的样本聚在一起，从而形成一个</span><span
                    class="inline-wrap"><b>类簇（cluster）</b></span><span class="inline-wrap">。那首先的问题是如何来</span><span
                    class="inline-wrap"><b>度量相似性</b></span><span class="inline-wrap">（similarity
                    measure）呢？这便是</span><span class="inline-wrap"><b>距离度量</b></span><span
                    class="inline-wrap">，在生活中我们说差别小则相似，对应到多维样本，每个样本可以对应于高维空间中的一个数据点，若它们的距离相近，我们便可以称它们相似。那接着如何来评价聚类结果的好坏呢？这便是</span><span
                    class="inline-wrap"><b>性能度量</b></span><span class="inline-wrap">，性能度量为评价聚类结果的好坏提供了一系列有效性指标。</span>
            </div>
        </div>
        <h2 id="gkju3BKP3jU64sbr4QJsWq" class="wolai-block"><span class="inline-wrap"><b>10.1 距离度量</b></span></h2>
        <div id="6Rz1vq9k9eQfB8WYCCHP1q" class="wolai-block wolai-text">
            <div><span
                    class="inline-wrap">谈及距离度量，最熟悉的莫过于欧式距离了，从年头一直用到年尾的距离计算公式：即对应属性之间相减的平方和再开根号。度量距离还有其它的很多经典方法，通常它们需要满足一些基本性质：</span>
            </div>
        </div>
        <div id="ghzWSPML9fmmzphqwxk7Wm" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84ed4c0390.png"
                    style="width: 725px" /></figure>
        </div>
        <div id="73dDPzabfdHYkF7UMETrmd" class="wolai-block wolai-text">
            <div><span class="inline-wrap">最常用的距离度量方法是</span><span class="inline-wrap"><b>“闵可夫斯基距离”（Minkowski
                        distance)</b></span><span class="inline-wrap">：</span></div>
        </div>
        <div id="34se2xu6Q4ofeeE3LAknSt" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84ed49e31f.png"
                    style="width: 390px" /></figure>
        </div>
        <div id="tDASLHZ4yUCJZpB8PR6wXZ" class="wolai-block wolai-text">
            <div><span class="inline-wrap">当<span class="jill"></span>p=1<span
                        class="jill"></span>时，闵可夫斯基距离即</span><span class="inline-wrap"><b>曼哈顿距离（Manhattan
                        distance）</b></span><span class="inline-wrap">：</span></div>
        </div>
        <div id="dRuPZoFDqFiQSjqK2GpuUt" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84ed49c31f.png"
                    style="width: 430px" /></figure>
        </div>
        <div id="hMeiDw49s3BAFj4Firr6EB" class="wolai-block wolai-text">
            <div><span class="inline-wrap">当<span class="jill"></span>p=2<span
                        class="jill"></span>时，闵可夫斯基距离即</span><span class="inline-wrap"><b>欧氏距离（Euclidean
                        distance）</b></span><span class="inline-wrap">：</span></div>
        </div>
        <div id="th1wGTxwTgLeE6nmrc3zLb" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84ed497613.png"
                    style="width: 441px" /></figure>
        </div>
        <div id="pEWsufkxtZGSowvyZM41j1" class="wolai-block wolai-text">
            <div><span class="inline-wrap">我们知道属性分为两种：</span><span class="inline-wrap"><b>连续属性</b></span><span
                    class="inline-wrap">和</span><span class="inline-wrap"><b>离散属性</b></span><span
                    class="inline-wrap">（有限个取值）。对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；而对于离散值的属性，需要作下面进一步的处理：</span>
            </div>
        </div>
        <blockquote id="x5td6Jq3Cf7FrSMM3EChyn" class="wolai-block"><span class="inline-wrap">若属性值之间</span><span
                class="inline-wrap"><b>存在序关系</b></span><span
                class="inline-wrap">，则可以将其转化为连续值，例如：身高属性“高”“中等”“矮”，可转化为<span class="jill"></span>{1, 0.5, 0}。</span>
        </blockquote>
        <blockquote id="hiTpMfoEcwuQ9wAuMjyrgx" class="wolai-block"><span class="inline-wrap">若属性值之间</span><span
                class="inline-wrap"><b>不存在序关系</b></span><span class="inline-wrap">，则通常将其转化为向量的形式，例如：性别属性“男”“女”，可转化为<span
                    class="jill"></span>{（1,0），（0,1）}。</span></blockquote>
        <div id="qG3UiAPyWEYcEpEMy1wqzD" class="wolai-block wolai-text">
            <div><span class="inline-wrap">在进行距离度量时，易知</span><span
                    class="inline-wrap"><b>连续属性和存在序关系的离散属性都可以直接参与计算</b></span><span
                    class="inline-wrap">，因为它们都可以反映一种程度，我们称其为“</span><span class="inline-wrap"><b>有序属性</b></span><span
                    class="inline-wrap">”；而对于不存在序关系的离散属性，我们称其为：“</span><span class="inline-wrap"><b>无序属性</b></span><span
                    class="inline-wrap">”，显然无序属性再使用闵可夫斯基距离就行不通了。</span></div>
        </div>
        <div id="rpNnmphQT1vD9zW8r31j8t" class="wolai-block wolai-text">
            <div><span class="inline-wrap"><b>对于无序属性，我们一般采用<span class="jill"></span>VDM<span
                            class="jill"></span>进行距离的计算</b></span><span class="inline-wrap">，例如：对于离散属性的两个取值<span
                        class="jill"></span>a<span class="jill"></span>和<span class="jill"></span>b，定义：</span></div>
        </div>
        <div id="i7bEo9ZwuxM3NuimAdmyzc" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84ed4e9560.png"
                    style="width: 533px" /></figure>
        </div>
        <div id="pVejBB4ffwdeHhYTcXD2pn" class="wolai-block wolai-text">
            <div><span class="inline-wrap">于是，在计算两个样本之间的距离时，我们可以将闵可夫斯基距离和<span class="jill"></span>VDM<span
                        class="jill"></span>混合在一起进行计算：</span></div>
        </div>
        <div id="3VKojwfb4rW7Ka7BKGFvDQ" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84ed507bc7.png"
                    style="width: 611px" /></figure>
        </div>
        <div id="piZos1xBvWomF9iP7wEMsP" class="wolai-block wolai-text">
            <div><span
                    class="inline-wrap">若我们定义的距离计算方法是用来度量相似性，例如下面将要讨论的聚类问题，即距离越小，相似性越大，反之距离越大，相似性越小。这时距离的度量方法并不一定需要满足前面所说的四个基本性质，这样的方法称为：</span><span
                    class="inline-wrap"><b>非度量距离（non-metric distance）</b></span><span class="inline-wrap">。</span></div>
        </div>
        <h2 id="pYLwswa4XV6FnoKt47rmnw" class="wolai-block"><span class="inline-wrap"><b>10.2 性能度量</b></span></h2>
        <div id="qJEuy7VkhnuJvtRQoxMEJ3" class="wolai-block wolai-text">
            <div><span
                    class="inline-wrap">由于聚类算法不依赖于样本的真实类标，就不能像监督学习的分类那般，通过计算分对分错（即精确度或错误率）来评价学习器的好坏或作为学习过程中的优化目标。一般聚类有两类性能度量指标：</span><span
                    class="inline-wrap"><b>外部指标</b></span><span class="inline-wrap">和</span><span
                    class="inline-wrap"><b>内部指标</b></span><span class="inline-wrap">。</span></div>
        </div>
        <h3 id="naDxdj6BXtXFyNEwUWf29y" class="wolai-block"><span class="inline-wrap"><b>10.2.1 外部指标</b></span></h3>
        <div id="st5rWVfWqniMaJfA2eXPnq" class="wolai-block wolai-text">
            <div><span class="inline-wrap">即将聚类结果与某个参考模型的结果进行比较，</span><span
                    class="inline-wrap"><b>以参考模型的输出作为标准，来评价聚类好坏</b></span><span class="inline-wrap">。假设聚类给出的结果为<span
                        class="jill"></span>λ，参考模型给出的结果是<span class="jill"></span>λ*，则我们将样本进行两两配对，定义：</span></div>
        </div>
        <div id="sSk16o93nXuc3EW3VvbTby" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84ed59160e.png"
                    style="width: 847px" /></figure>
        </div>
        <div id="4nQ6RxPUkLmrNPraqXkSX7" class="wolai-block wolai-text">
            <div><span class="inline-wrap">显然<span class="jill"></span>a<span class="jill"></span>和<span
                        class="jill"></span>b<span class="jill"></span>代表着聚类结果好坏的正能量，b<span class="jill"></span>和<span
                        class="jill"></span>c<span class="jill"></span>则表示参考结果和聚类结果相矛盾，基于这四个值可以导出以下常用的外部评价指标：</span>
            </div>
        </div>
        <div id="7BtewaiHcnkb1B1C7khKLT" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84ed587438.png"
                    style="width: 716px" /></figure>
        </div>
        <h3 id="naDxdj6BXtXFyNEwUWf29y" class="wolai-block"><span class="inline-wrap"><b>10.2.2 内部指标</b></span></h3>
        <div id="aEz5kdGQaNu6zPKUoPQ4Zv" class="wolai-block wolai-text">
            <div><span
                    class="inline-wrap">内部指标即不依赖任何外部模型，直接对聚类的结果进行评估，聚类的目的是想将那些相似的样本尽可能聚在一起，不相似的样本尽可能分开，直观来说：</span><span
                    class="inline-wrap"><b>簇内高内聚紧紧抱团，簇间低耦合老死不相往来</b></span><span class="inline-wrap">。定义：</span></div>
        </div>
        <div id="6VtxiZK9eb4HRNKywfQe1" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84ed581852.png"
                    style="width: 702px" /></figure>
        </div>
        <div id="fWEL2tNzNnvx68zxVRuvGT" class="wolai-block wolai-text">
            <div><span class="inline-wrap">基于上面的四个距离，可以导出下面这些常用的内部评价指标：</span></div>
        </div>
        <div id="o35spkji9yDw2smwxLaBCz" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84ed582854.png"
                    style="width: 687px" /></figure>
        </div>
        <h2 id="2bJ7GzCjmEt5KRFZs4Bdwm" class="wolai-block"><span class="inline-wrap"><b>10.3 原型聚类</b></span></h2>
        <div id="vwcWEuFd66ZSMLYUXQh6Uh" class="wolai-block wolai-text">
            <div><span class="inline-wrap">原型聚类即“</span><span class="inline-wrap"><b>基于原型的聚类</b></span><span
                    class="inline-wrap">”（prototype-based clustering），原型表示模板的意思，就是通过参考一个模板向量或模板分布的方式来完成聚类的过程，常见的<span
                        class="jill"></span>K-Means<span class="jill"></span>便是基于簇中心来实现聚类，混合高斯聚类则是基于簇分布来实现聚类。</span>
            </div>
        </div>
        <h3 id="vEJ3ETER9iaHtEJMJTj4Ti" class="wolai-block"><span class="inline-wrap"><b>10.3.1 K-Means</b></span></h3>
        <div id="tZbFuioTpmqFbaFMLgK61n" class="wolai-block wolai-text">
            <div><span class="inline-wrap">K-Means<span class="jill"></span>的思想十分简单，</span><span
                    class="inline-wrap"><b>首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛</b></span><span
                    class="inline-wrap">。但是其中迭代的过程并不是主观地想象得出，事实上，若将样本的类别看做为“隐变量”（latent
                    variable），类中心看作样本的分布参数，这一过程正是通过</span><span class="inline-wrap"><b>EM<span
                            class="jill"></span>算法</b></span><span
                    class="inline-wrap">的两步走策略而计算出，其根本的目的是为了最小化平方误差函数<span class="jill"></span>E：</span></div>
        </div>
        <div id="fghewBNmwMgjExMwwEdE9Q" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84fb82b5d3.png"
                    style="width: 246px" /></figure>
        </div>
        <div id="7mSNyfBdgRrY2iX1NSusFm" class="wolai-block wolai-text">
            <div><span class="inline-wrap">K-Means<span class="jill"></span>的算法流程如下所示：</span></div>
        </div>
        <div id="aYFpTYJi3XNErTu3QVcVjs" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84fb9c0817.png"
                    style="width: 665px" /></figure>
        </div>
        <h3 id="a415YzUNFeCD2QAZF7ipFZ" class="wolai-block"><span class="inline-wrap"><b>10.3.2 学习向量量化（LVQ）</b></span>
        </h3>
        <div id="wLSiBqHwaWjKc4jkwC4RCL" class="wolai-block wolai-text">
            <div><span class="inline-wrap">LVQ<span class="jill"></span>也是基于原型的聚类算法，与<span
                        class="jill"></span>K-Means<span class="jill"></span>不同的是，</span><span
                    class="inline-wrap"><b>LVQ<span class="jill"></span>使用样本真实类标记辅助聚类</b></span><span
                    class="inline-wrap">，首先<span class="jill"></span>LVQ<span
                        class="jill"></span>根据样本的类标记，从各类中分别随机选出一个样本作为该类簇的原型，从而组成了一个</span><span
                    class="inline-wrap"><b>原型特征向量组</b></span><span
                    class="inline-wrap">，接着从样本集中随机挑选一个样本，计算其与原型向量组中每个向量的距离，并选取距离最小的原型向量所在的类簇作为它的划分结果，再与真实类标比较。</span>
            </div>
        </div>
        <blockquote id="ftv92gQmDvzkznxtPkbiWy" class="wolai-block"><span
                class="inline-wrap"><b>若划分结果正确，则对应原型向量向这个样本靠近一些</b></span></blockquote>
        <blockquote id="tXgnPrTs5eFXHgrzTb3i6d" class="wolai-block"><span
                class="inline-wrap"><b>若划分结果不正确，则对应原型向量向这个样本远离一些</b></span></blockquote>
        <div id="aknY1DxEy3WskG4bv6B2eG" class="wolai-block wolai-text">
            <div><span class="inline-wrap">LVQ<span class="jill"></span>算法的流程如下所示：</span></div>
        </div>
        <div id="jMinuGzHBZ4AJNu3UAYyuT" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84fb9d59f2.png"
                    style="width: 608px" /></figure>
        </div>
        <h3 id="x56wXDPuEMackf81LjjZoJ" class="wolai-block"><span class="inline-wrap"><b>10.3.3 高斯混合聚类</b></span></h3>
        <div id="7VobBovG6oLq4iofCVE72a" class="wolai-block wolai-text">
            <div><span class="inline-wrap">现在可以看出<span class="jill"></span>K-Means<span class="jill"></span>与<span
                        class="jill"></span>LVQ<span
                        class="jill"></span>都试图以类中心作为原型指导聚类，高斯混合聚类则采用高斯分布来描述原型。现假设</span><span
                    class="inline-wrap"><b>每个类簇中的样本都服从一个多维高斯分布，那么空间中的样本可以看作由<span class="jill"></span>k<span
                            class="jill"></span>个多维高斯分布混合而成</b></span><span class="inline-wrap">。</span></div>
        </div>
        <div id="3zLPqGvukng9BnWJ93Q1Ce" class="wolai-block wolai-text">
            <div><span class="inline-wrap">对于多维高斯分布，其概率密度函数如下所示：</span></div>
        </div>
        <div id="jEQNi8zhrWrVp7oBrcDd8E" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84fb870d98.png"
                    style="width: 343px" /></figure>
        </div>
        <div id="hAD11TKrdu1torZUhqCY6S" class="wolai-block wolai-text">
            <div><span class="inline-wrap">其中<span class="jill"></span>u<span
                        class="jill"></span>表示均值向量，∑表示协方差矩阵，可以看出一个多维高斯分布完全由这两个参数所确定。接着定义高斯混合分布为：</span></div>
        </div>
        <div id="7HGdKExkvBHon3ubXsAicX" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84fb876794.png"
                    style="width: 280px" /></figure>
        </div>
        <div id="mYBxmUttD2woMzmKQMgQjH" class="wolai-block wolai-text">
            <div><span class="inline-wrap">α<span class="jill"></span>称为混合系数，这样空间中样本的采集过程则可以抽象为：</span><span
                    class="inline-wrap"><b>（1）先选择一个类簇（高斯分布），（2）再根据对应高斯分布的密度函数进行采样</b></span><span
                    class="inline-wrap">，这时候贝叶斯公式又能大展身手了：</span></div>
        </div>
        <div id="qmaucMXMkN49qtgVp8tWjL" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84fb9191d9.png"
                    style="width: 433px" /></figure>
        </div>
        <div id="vesUPttBVRyh572ympTjSA" class="wolai-block wolai-text">
            <div><span class="inline-wrap">此时只需要选择<span class="jill"></span>PM<span
                        class="jill"></span>最大时的类簇并将该样本划分到其中，看到这里很容易发现：这和那个传说中的贝叶斯分类不是神似吗，都是通过贝叶斯公式展开，然后计算类先验概率和类条件概率。但遗憾的是：</span><span
                    class="inline-wrap"><b>这里没有真实类标信息，对于类条件概率，并不能像贝叶斯分类那样通过最大似然法美好地计算出来</b></span><span
                    class="inline-wrap">，因为这里的样本可能属于所有的类簇，这里的似然函数变为：</span></div>
        </div>
        <div id="dfHmvs1vmZ7dHtojET34zi" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84fb871d4a.png"
                    style="width: 544px" /></figure>
        </div>
        <div id="7r3migxfkKpahRpmvrRbZA" class="wolai-block wolai-text">
            <div><span class="inline-wrap">可以看出：简单的最大似然法根本无法求出所有的参数，这样<span class="jill"></span>PM<span
                        class="jill"></span>也就没法计算。</span><span class="inline-wrap"><b>这里就要召唤出之前的<span
                            class="jill"></span>EM<span class="jill"></span>大法，首先对高斯分布的参数及混合系数进行随机初始化，计算出各个<span
                            class="jill"></span>PM（即<span class="jill"></span>γji，第<span class="jill"></span>i<span
                            class="jill"></span>个样本属于<span class="jill"></span>j<span
                            class="jill"></span>类），再最大化似然函数（即<span class="jill"></span>LL（D）分别对<span
                            class="jill"></span>α、u<span class="jill"></span>和∑求偏导 ），对参数进行迭代更新</b></span><span
                    class="inline-wrap">。</span></div>
        </div>
        <div id="czCLteQBGYbbMA6L7eq5x5" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84fb8a6f32.png"
                    style="width: 472px" /></figure>
        </div>
        <div id="wuShc6iFqzsYeM5VXzmqc3" class="wolai-block wolai-text">
            <div><span class="inline-wrap">高斯混合聚类的算法流程如下图所示：</span></div>
        </div>
        <div id="rZfstQr9oUwutmJczXkw3f" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84fb9c4fa4.png"
                    style="width: 635px" /></figure>
        </div>
        <h2 id="gCg6KHPKkoGYeXv4wQ5xQV" class="wolai-block"><span class="inline-wrap"><b>10.4 密度聚类</b></span></h2>
        <div id="2hRTWdGxrndKke854UdJ6J" class="wolai-block wolai-text">
            <div><span
                    class="inline-wrap">密度聚类则是基于密度的聚类，它从样本分布的角度来考察样本之间的可连接性，并基于可连接性（密度可达）不断拓展疆域（类簇）。其中最著名的便是</span><span
                    class="inline-wrap"><b>DBSCAN</b></span><span class="inline-wrap">算法，首先定义以下概念：</span></div>
        </div>
        <div id="vbQ4UX9ZestG8trqdusrWu" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc84fb9bd69c.png"
                    style="width: 682px" /></figure>
        </div>
        <div id="iS5bxFxFNqsoMR85Wd9zAk" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc8509f8d619.png"
                    style="width: 555px" /></figure>
        </div>
        <div id="cbGuFc3MwYU7HPweoEeRwG" class="wolai-block wolai-text">
            <div><span class="inline-wrap">简单来理解<span class="jill"></span>DBSCAN<span
                        class="jill"></span>便是：</span><span
                    class="inline-wrap"><b>找出一个核心对象所有密度可达的样本集合形成簇</b></span><span
                    class="inline-wrap">。首先从数据集中任选一个核心对象<span class="jill"></span>A，找出所有<span class="jill"></span>A<span
                        class="jill"></span>密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。DBSCAN<span
                        class="jill"></span>算法的流程如下图所示：</span></div>
        </div>
        <div id="tN8RsmM1KYXQRCxrmyYaYn" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc8509feb587.png"
                    style="width: 670px" /></figure>
        </div>
        <h2 id="aZXg8LXAch6GCGJvFvW3U" class="wolai-block"><span class="inline-wrap"><b>10.5 层次聚类</b></span></h2>
        <div id="rqShGgY4iechHi6WcAhhHd" class="wolai-block wolai-text">
            <div><span class="inline-wrap">层次聚类是一种基于树形结构的聚类方法，常用的是</span><span
                    class="inline-wrap"><b>自底向上</b></span><span class="inline-wrap">的结合策略（</span><span
                    class="inline-wrap"><b>AGNES<span class="jill"></span>算法</b></span><span
                    class="inline-wrap">）。假设有<span class="jill"></span>N<span class="jill"></span>个待聚类的样本，其基本步骤是：</span>
            </div>
        </div>
        <blockquote id="nVGQdp8Z2ova8foyRyX1oi" class="wolai-block"><span
                class="inline-wrap">把每个样本归为一类，计算每两个类之间的距离，也就是样本与样本之间的相似度；</span></blockquote>
        <blockquote id="nG7zadRyXCjX2mTYjpJKZL" class="wolai-block"><span
                class="inline-wrap">2.寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个）；</span></blockquote>
        <blockquote id="mWaMveoNodVXj7dz86vUaH" class="wolai-block"><span class="inline-wrap">3.重新计算新生成的这个</span><span
                class="inline-wrap"><b>类与各个旧类之间的相似度</b></span><span class="inline-wrap">；</span></blockquote>
        <blockquote id="fywzrrYnNyvTygt95DRMvH" class="wolai-block"><span class="inline-wrap">4.重复<span
                    class="jill"></span>2<span class="jill"></span>和<span class="jill"></span>3<span
                    class="jill"></span>直到所有样本点都归为一类，结束。</span></blockquote>
        <div id="uS37q4W63W4GQPmG4kd5fG" class="wolai-block wolai-text">
            <div><span class="inline-wrap">可以看出其中最关键的一步就是</span><span class="inline-wrap"><b>计算两个类簇的相似度</b></span><span
                    class="inline-wrap">，这里有多种度量方法：</span></div>
            <ul class="wolai-block">
                <li id="rjnd5gyiodjbmnpr5AT7qK">
                    <div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor"
                            xmlns="http://www.w3.org/2000/svg">
                            <path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path>
                        </svg></div><span class="inline-wrap">单链接（single-linkage）:取类间最小距离。</span>
                </li>
            </ul>
        </div>
        <div id="bwwPfqR2smb8vrP59N88My" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc8509ebb022.png"
                    style="width: 404px" /></figure>
        </div>
        <ul class="wolai-block">
            <li id="tMnfNK8FAFFbP6M3r8YMsd">
                <div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor"
                        xmlns="http://www.w3.org/2000/svg">
                        <path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path>
                    </svg></div><span class="inline-wrap">全链接（complete-linkage）:取类间最大距离</span>
            </li>
        </ul>
        <div id="anCvzGqv6zE5JYYr2sXHH5" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc8509eb2b30.png"
                    style="width: 397px" /></figure>
        </div>
        <ul class="wolai-block">
            <li id="4cdo4NxbkE6S2qWbKr4jZr">
                <div class="marker"><svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor"
                        xmlns="http://www.w3.org/2000/svg">
                        <path d="M12 14.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"></path>
                    </svg></div><span class="inline-wrap">均链接（average-linkage）:取类间两两的平均距离</span>
            </li>
        </ul>
        <div id="7tsagzPM3G3MvDsgcDxyFF" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc8509f089a7.png"
                    style="width: 460px" /></figure>
        </div>
        <div id="x1CkrGx3h63tfQUMBux38A" class="wolai-block wolai-text">
            <div><span class="inline-wrap">很容易看出：</span><span
                    class="inline-wrap"><b>单链接的包容性极强，稍微有点暧昧就当做是自己人了，全链接则是坚持到底，只要存在缺点就坚决不合并，均连接则是从全局出发顾全大局</b></span><span
                    class="inline-wrap">。层次聚类法的算法流程如下所示：</span></div>
        </div>
        <div id="2iifkwpCsdAqGh3kS4NjGv" class="wolai-block wolai-center">
            <figure style="width: 100%"><img src="https://i.loli.net/2018/10/18/5bc8509f9d4a0.png"
                    style="width: 442px" /></figure>
        </div>
        <blockquote id="MziXauCAcaS8gyGMHwJef" class="wolai-block"><span
                class="inline-wrap">在此聚类算法就介绍完毕，分类/聚类都是机器学习中最常见的任务，我实验室的大<span class="jill"></span>Boss<span
                    class="jill"></span>也是靠着聚类起家，从此走上人生事业钱途...之巅峰，在书最后的阅读材料还看见<span class="jill"></span>Boss<span
                    class="jill"></span>的名字，所以这章也是必读不可了...</span></blockquote>
    </article>
    <div class="wolai-sub-page wolai-block" style="background-color: white;">
        <div data-symbol="ℹ️" class="page-icon" style="float: left;">
            <a
            href="/machineLearning/ML/周志华《Machine Learning》学习笔记(10)--集成学习.html"><span>第十章 集成学习</span></a>
            <a href="/machineLearning/machineLearning.html"><span>主目录</span></a>
            <a
            href="/machineLearning/ML/周志华《Machine Learning》学习笔记(12)--降维与度量学习.html"><span>第十二章 降维与度量学习</span></a>
        </div>
    </div>
    
    <footer></footer>
</body>

</html></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/favicon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">tiaotiaohu</div><div class="author-info__description">跳跳虎的酒吧</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">归档</div><div class="length-num">18</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/tiaotiaohu98/"><i class="fab fa-github"></i><span>我的 Github</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">跳跳虎的个人博客</div></div><div class="card-widget"><div class="item-headline"><i></i><span></span></div><div class="item-content"></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#iJUbhgr5Y8zYmocGeBBmzP"><span class="toc-number">1.</span> <span class="toc-text">10、聚类算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#gkju3BKP3jU64sbr4QJsWq"><span class="toc-number">1.1.</span> <span class="toc-text">10.1 距离度量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pYLwswa4XV6FnoKt47rmnw"><span class="toc-number">1.2.</span> <span class="toc-text">10.2 性能度量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#naDxdj6BXtXFyNEwUWf29y"><span class="toc-number">1.2.1.</span> <span class="toc-text">10.2.1 外部指标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#naDxdj6BXtXFyNEwUWf29y"><span class="toc-number">1.2.2.</span> <span class="toc-text">10.2.2 内部指标</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2bJ7GzCjmEt5KRFZs4Bdwm"><span class="toc-number">1.3.</span> <span class="toc-text">10.3 原型聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#vEJ3ETER9iaHtEJMJTj4Ti"><span class="toc-number">1.3.1.</span> <span class="toc-text">10.3.1 K-Means</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a415YzUNFeCD2QAZF7ipFZ"><span class="toc-number">1.3.2.</span> <span class="toc-text">10.3.2 学习向量量化（LVQ）
        </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#x56wXDPuEMackf81LjjZoJ"><span class="toc-number">1.3.3.</span> <span class="toc-text">10.3.3 高斯混合聚类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gCg6KHPKkoGYeXv4wQ5xQV"><span class="toc-number">1.4.</span> <span class="toc-text">10.4 密度聚类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#aZXg8LXAch6GCGJvFvW3U"><span class="toc-number">1.5.</span> <span class="toc-text">10.5 层次聚类</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/08/29/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(10)--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" title="第10节_集成学习"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第10节_集成学习"/></a><div class="content"><a class="title" href="/2022/08/29/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(10)--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" title="第10节_集成学习">第10节_集成学习</a><time datetime="2022-08-29T07:18:04.072Z" title="发表于 2022-08-29 15:18:04">2022-08-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/29/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(11)--%E8%81%9A%E7%B1%BB/" title="第11节_聚类"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第11节_聚类"/></a><div class="content"><a class="title" href="/2022/08/29/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(11)--%E8%81%9A%E7%B1%BB/" title="第11节_聚类">第11节_聚类</a><time datetime="2022-08-29T07:18:04.072Z" title="发表于 2022-08-29 15:18:04">2022-08-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/29/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(13)--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" title="第13节_特征选择与稀疏学习"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第13节_特征选择与稀疏学习"/></a><div class="content"><a class="title" href="/2022/08/29/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(13)--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/" title="第13节_特征选择与稀疏学习">第13节_特征选择与稀疏学习</a><time datetime="2022-08-29T07:18:04.072Z" title="发表于 2022-08-29 15:18:04">2022-08-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/29/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(14)--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" title="第14节_计算学习理论"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第14节_计算学习理论"/></a><div class="content"><a class="title" href="/2022/08/29/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(14)--%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/" title="第14节_计算学习理论">第14节_计算学习理论</a><time datetime="2022-08-29T07:18:04.072Z" title="发表于 2022-08-29 15:18:04">2022-08-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/29/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(15)--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" title="第15节_半监督学习"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第15节_半监督学习"/></a><div class="content"><a class="title" href="/2022/08/29/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(15)--%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" title="第15节_半监督学习">第15节_半监督学习</a><time datetime="2022-08-29T07:18:04.072Z" title="发表于 2022-08-29 15:18:04">2022-08-29</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="card-category-list-name">机器学习笔记</span><span class="card-category-list-count">17</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%91%A8%E5%BF%97%E5%8D%8E/" style="font-size: 1.15em; color: rgb(103, 26, 54)">机器学习笔记_周志华</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/08/"><span class="card-archive-list-date">八月 2022</span><span class="card-archive-list-count">17</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/04/"><span class="card-archive-list-date">四月 2022</span><span class="card-archive-list-count">1</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">18</div></div><div class="webinfo-item"><div class="item-name">已运行时间 :</div><div class="item-count" id="runtimeshow" data-publishDate="2022-04-07T16:00:00.000Z"></div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-09-01T13:33:04.586Z"></div></div></div></div><div class="card-widget user-map" id="user-map"><div class="item-headline"><i class="fas fa-heartbeat"></i><span>访客地图</span></div><div class="item-content"><script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=5V2tOKp8qAdRM-i8eu7ETTO9ugt5uKbbG-U7Yj8uMl8"></script></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By tiaotiaohu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">欢迎来到我的博客!!!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>